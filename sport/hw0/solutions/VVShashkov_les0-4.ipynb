{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score , StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['Word'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([df, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2,5), max_features = 70, encoding='utf8')\n",
    "\n",
    "for_tf_idf = bigram_vectorizer.fit_transform(all_data.Word.apply(str.lower).tolist())\n",
    "tfidf_sparse = TfidfTransformer().fit_transform(for_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(tfidf_sparse.toarray() \n",
    "                     , index=all_data.index, columns=['tf_idf_' + str(i) for i in range(tfidf_sparse.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([all_data, tfidf], axis=1)\n",
    "#df_test = pd.concat([df_test, tfidf.iloc[df_test.index].Word], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['fl_is_ip'] = all_data.Word.apply(lambda x: x[0].isupper()).map({False: 0, True: 1})\n",
    "#df_test['fl_is_ip'] = df_test.Word.apply(lambda x: x[0].isupper()).map({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cnt_up'] = all_data.Word.apply(lambda x: sum(1 if x[i].isupper() else 0 for i in range(len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_upcase(word):\n",
    "    if len(word) > 1:\n",
    "        if word[1] == word[1].upper():\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['sl_is_ip'] = all_data.Word.map(is_upcase)\n",
    "#df_test['sl_is_ip'] = df_test.Word.map(is_upcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['title'] = all_data.Word.apply(lambda x: x.istitle()).map({False: 0, True: 1})\n",
    "#df_test['title'] = df_test.Word.apply(lambda x: x.istitle()).map({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(line):\n",
    "    line.rstrip()\n",
    "    vowel_list = ('А', 'Е', 'Ё', 'И', 'О', 'У', 'Ы', 'Э', 'Ю', 'Я')\n",
    "    k = 0 #счётчик гласных в слове\n",
    "    #считаем количество гласных в слове\n",
    "    for symbol in line:\n",
    "        if vowel_list.__contains__(symbol.upper()):\n",
    "            k+=1\n",
    "        #Добавляем полученное число в список\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['slog'] = all_data.Word.map(get_syllables)\n",
    "#df_test['slog'] = df_test.Word.map(get_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['lastsym'] = all_data.Word.apply(lambda x: ord(x[-1]))\n",
    "#df_test['lastsym'] = df_test['Word'].apply(lambda x: ord(x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['prelastsym'] = all_data.Word.apply(lambda x: ord(x[-2]) if len(x)>1 else 100)\n",
    "#df_test['prelastsym'] = df_test['Word'].apply(lambda x: ord(x[-2]) if len(x)>1 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NamesExtractor\n",
    "extractor = NamesExtractor()\n",
    "\n",
    "def has_name(text):\n",
    "    matches = extractor(text)\n",
    "    return 0 if matches.as_json==[] else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['а',  'я', 'ё', 'у','е', 'о', 'э', 'ю', 'и', 'ы', 'Ё', 'У', 'Е', 'Ы','А', 'О', 'Э', 'Ю', 'И', 'Я']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    df['Lenght'] = df['Word'].apply(lambda x: len(x))\n",
    "    df['Vowels'] = df['Word'].apply(lambda x: sum(1 if l in vowels else 0 for l in x))\n",
    "    df['Consonants'] = df['Lenght'] - df['Vowels']\n",
    "    df['Not_null_Consonants'] = df['Consonants'].apply(lambda x: 0.001 if x==0 else x)\n",
    "    df['Vow/Conson'] = df['Vowels'] / df['Not_null_Consonants']\n",
    "    df = df.drop(columns=['Not_null_Consonants'])\n",
    "    df['is_lower'] = df['Word'].apply(lambda x: 1 if x[0] == x[0].lower() else 0)\n",
    "    #df['has_name'] = df['Word'].apply(lambda word: has_name(word))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = prepare_features(all_data)\n",
    "#test_all = prepare_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymorphy2\n",
    "\n",
    "# probability score threshold\n",
    "prob_thresh = 0.4\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_name_pymorphy(text):\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        for p in morph.parse(word):\n",
    "            if 'Name' in p.tag and p.score >= prob_thresh:\n",
    "                return p.score\n",
    "            else:\n",
    "                return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['mrph'] = all_data.Word.apply(lambda x: is_name_pymorphy(x))\n",
    "#df_test['mrph'] = df_test.Word.apply(lambda x: is_name_pymorphy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['has_name'] = all_data['Word'].apply(lambda word: has_name(word))\n",
    "#df_test['has_name'] = df_test['Word'].apply(lambda word: has_name(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Consonants',\n",
       " 'Lenght',\n",
       " 'Vow/Conson',\n",
       " 'Vowels',\n",
       " 'cnt_up',\n",
       " 'fl_is_ip',\n",
       " 'has_name',\n",
       " 'is_lower',\n",
       " 'lastsym',\n",
       " 'mrph',\n",
       " 'prelastsym',\n",
       " 'sl_is_ip',\n",
       " 'slog',\n",
       " 'tf_idf_0',\n",
       " 'tf_idf_1',\n",
       " 'tf_idf_10',\n",
       " 'tf_idf_11',\n",
       " 'tf_idf_12',\n",
       " 'tf_idf_13',\n",
       " 'tf_idf_14',\n",
       " 'tf_idf_15',\n",
       " 'tf_idf_16',\n",
       " 'tf_idf_17',\n",
       " 'tf_idf_18',\n",
       " 'tf_idf_19',\n",
       " 'tf_idf_2',\n",
       " 'tf_idf_20',\n",
       " 'tf_idf_21',\n",
       " 'tf_idf_22',\n",
       " 'tf_idf_23',\n",
       " 'tf_idf_24',\n",
       " 'tf_idf_25',\n",
       " 'tf_idf_26',\n",
       " 'tf_idf_27',\n",
       " 'tf_idf_28',\n",
       " 'tf_idf_29',\n",
       " 'tf_idf_3',\n",
       " 'tf_idf_30',\n",
       " 'tf_idf_31',\n",
       " 'tf_idf_32',\n",
       " 'tf_idf_33',\n",
       " 'tf_idf_34',\n",
       " 'tf_idf_35',\n",
       " 'tf_idf_36',\n",
       " 'tf_idf_37',\n",
       " 'tf_idf_38',\n",
       " 'tf_idf_39',\n",
       " 'tf_idf_4',\n",
       " 'tf_idf_40',\n",
       " 'tf_idf_41',\n",
       " 'tf_idf_42',\n",
       " 'tf_idf_43',\n",
       " 'tf_idf_44',\n",
       " 'tf_idf_45',\n",
       " 'tf_idf_46',\n",
       " 'tf_idf_47',\n",
       " 'tf_idf_48',\n",
       " 'tf_idf_49',\n",
       " 'tf_idf_5',\n",
       " 'tf_idf_50',\n",
       " 'tf_idf_51',\n",
       " 'tf_idf_52',\n",
       " 'tf_idf_53',\n",
       " 'tf_idf_54',\n",
       " 'tf_idf_55',\n",
       " 'tf_idf_56',\n",
       " 'tf_idf_57',\n",
       " 'tf_idf_58',\n",
       " 'tf_idf_59',\n",
       " 'tf_idf_6',\n",
       " 'tf_idf_60',\n",
       " 'tf_idf_61',\n",
       " 'tf_idf_62',\n",
       " 'tf_idf_63',\n",
       " 'tf_idf_64',\n",
       " 'tf_idf_65',\n",
       " 'tf_idf_66',\n",
       " 'tf_idf_67',\n",
       " 'tf_idf_68',\n",
       " 'tf_idf_69',\n",
       " 'tf_idf_7',\n",
       " 'tf_idf_8',\n",
       " 'tf_idf_9',\n",
       " 'title'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_columns = set(all_data.columns)\n",
    "columns_transformed = set(('Word',))\n",
    "\n",
    "target_column = set(('Label',))\n",
    "train_columns -= columns_transformed\n",
    "train_columns -= target_column\n",
    "train_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = sorted(list(train_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = all_data[all_data['Label'].notnull()]\n",
    "new_test = all_data[all_data['Label'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = imputer.transform(new_test[train_columns])\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='median')\n",
    "X_train = imputer.fit_transform(new_train[train_columns])\n",
    "X_test = imputer.transform(new_test[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df[train_columns]\n",
    "y = new_train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(random_state=42, oob_score=True, n_jobs=24)\n",
    "RF.fit(X_train_scaled, y)\n",
    "RF.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print( cross_val_score(RF, X=X_train_scaled, y=y, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = RF.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(data={\"Id\":df_test.index,\"Prediction\":probas}) \n",
    "df_to_save.to_csv(\"submission.csv\", sep=',' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(RF, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xr = xgboost.XGBRegressor()\n",
    "xr.fit(X_train_scaled, y)\n",
    "#xr.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( cross_val_score(xr, X=X_train_scaled, y=y, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probasxr = xr.predict(X_test_scaled)\n",
    "#probas = RF.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probasxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "def validate(x , y):\n",
    "    model = XGBClassifier(max_depth = 10 , n_estimators=670 , learning_rate=0.09 , colsample_bytree=0.9 , colsample_bylevel=0.6)\n",
    "    cv = StratifiedKFold(4 ,shuffle=True, random_state=99)\n",
    "    score = cross_val_score(model , x , y , scoring='roc_auc' , cv=cv)\n",
    "    print (score.mean() , score.std() , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(X_train_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.6,\n",
       "       colsample_bytree=0.9, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=670,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(max_depth = 10 , n_estimators=670 , learning_rate=0.09 , colsample_bytree=0.9 , colsample_bylevel=0.6)\n",
    "model.fit(X_train_scaled , y)\n",
    "#sample['Prediction'] = model.predict_proba(X_train_scaled)[:,0]\n",
    "#sample.to_csv('submit.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "probasxr = model.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(data={\"Id\":new_test.index,\"Prediction\":probasxr[:,0]}) \n",
    "df_to_save.to_csv(\"submission.csv\", sep=',' , index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
